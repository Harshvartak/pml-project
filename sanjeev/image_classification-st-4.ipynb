{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41174d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def explore_and_visualize_digits(image_dataset_path, num_samples=25, subset_size=1000):\n",
    "    \"\"\"\n",
    "    Explore and visualize an unlabelled handwritten digits dataset.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_dataset_path (str): Path to the .npy file containing the dataset.\n",
    "    - num_samples (int): Number of sample images to display in the grid.\n",
    "    - subset_size (int): Size of subset for t-SNE visualization to reduce computation.\n",
    "    \n",
    "    Returns:\n",
    "    - None: Displays plots and prints analysis results.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load the dataset\n",
    "    Xtrain_img = np.load(image_dataset_path)\n",
    "    print(\"Dataset shape:\", Xtrain_img.shape)\n",
    "    print(\"Data type:\", Xtrain_img.dtype)\n",
    "    print(f\"Pixel value range: [{Xtrain_img.min():.2f}, {Xtrain_img.max():.2f}]\")\n",
    "    \n",
    "    # Reshape for visualization (remove channel dimension)\n",
    "    Xtrain_2d = Xtrain_img.reshape(-1, 28, 28)\n",
    "    print(\"Reshaped for visualization:\", Xtrain_2d.shape)\n",
    "    \n",
    "    # Compute basic statistics\n",
    "    pixel_mean = Xtrain_img.mean()\n",
    "    pixel_std = Xtrain_img.std()\n",
    "    print(f\"Mean pixel value: {pixel_mean:.4f}, Std: {pixel_std:.4f}\")\n",
    "    \n",
    "    # Plot pixel intensity distribution\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(Xtrain_img.flatten(), bins=50, kde=True)\n",
    "    plt.title(\"Pixel Intensity Distribution\")\n",
    "    plt.xlabel(\"Pixel Value\")\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize sample images\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    sample_indices = np.random.choice(Xtrain_2d.shape[0], num_samples, replace=False)\n",
    "    for i, idx in enumerate(sample_indices):\n",
    "        plt.subplot(5, 5, i + 1)\n",
    "        plt.imshow(Xtrain_2d[idx], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.suptitle(\"Sample Handwritten Digits\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Reshape for dimensionality reduction\n",
    "    Xtrain_flat = Xtrain_img.reshape(Xtrain_img.shape[0], -1)\n",
    "    print(\"Flattened shape for PCA/t-SNE:\", Xtrain_flat.shape)\n",
    "    \n",
    "    # Apply PCA for 2D visualization\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(Xtrain_flat)\n",
    "    explained_variance_ratio = pca.explained_variance_ratio_.sum()\n",
    "    print(f\"PCA: Explained variance ratio (2 components): {explained_variance_ratio:.4f}\")\n",
    "    \n",
    "    # Plot PCA results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5, s=10)\n",
    "    plt.title(\"PCA Projection of Handwritten Digits\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Apply t-SNE for 2D visualization (use a subset for speed)\n",
    "    subset_indices = np.random.choice(Xtrain_flat.shape[0], subset_size, replace=False)\n",
    "    X_subset = Xtrain_flat[subset_indices]\n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_subset)\n",
    "    \n",
    "    # Plot t-SNE results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(X_tsne[:, 0], X_tsne[:, 1], alpha=0.5, s=10)\n",
    "    plt.title(f\"t-SNE Projection of Handwritten Digits (Subset of {subset_size})\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import pandas as pd\n",
    "\n",
    "def cluster_digits_with_autoencoder(image_dataset_path, num_clusters=10, subset_size=20000):\n",
    "    \"\"\"\n",
    "    Cluster unlabelled handwritten digits using a convolutional autoencoder and compare with other algorithms.\n",
    "    \n",
    "    Parameters:\n",
    "    - image_dataset_path (str): Path to the .npy file containing the dataset.\n",
    "    - num_clusters (int): Number of clusters (default 10 for digits 0-9).\n",
    "    - subset_size (int): Size of subset for faster computation (default 20,000).\n",
    "    \n",
    "    Returns:\n",
    "    - results (dict): Dictionary with algorithm names and their results (labels, metrics).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load and preprocess the dataset\n",
    "    Xtrain_img = np.load(image_dataset_path)\n",
    "    print(\"Dataset shape:\", Xtrain_img.shape)\n",
    "    print(\"Data type:\", Xtrain_img.dtype)\n",
    "    print(f\"Pixel value range: [{Xtrain_img.min():.2f}, {Xtrain_img.max():.2f}]\")\n",
    "    \n",
    "    # Use a subset for faster computation\n",
    "    indices = np.random.choice(Xtrain_img.shape[0], subset_size, replace=False)\n",
    "    X_subset = Xtrain_img[indices]\n",
    "    X_flat = X_subset.reshape(subset_size, -1)\n",
    "    print(\"Subset shape (flattened):\", X_flat.shape)\n",
    "    \n",
    "    # Standardize features for clustering (raw data)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_flat)\n",
    "    \n",
    "    # Initialize results dictionary\n",
    "    results = {}\n",
    "    \n",
    "    # Autoencoder + K-means\n",
    "    print(\"Training Convolutional Autoencoder...\")\n",
    "    X_images = X_subset.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    # Build autoencoder\n",
    "    input_img = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = models.Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Define encoder and decoder\n",
    "    encoder = models.Model(input_img, encoded)\n",
    "    encoded_input = layers.Input(shape=(7, 7, 16))\n",
    "    x = autoencoder.layers[-5](encoded_input)\n",
    "    for layer in autoencoder.layers[-4:]:\n",
    "        x = layer(x)\n",
    "    decoder = models.Model(encoded_input, x)\n",
    "    \n",
    "    # Train autoencoder\n",
    "    autoencoder.fit(X_images, X_images, epochs=10, batch_size=128, verbose=1)\n",
    "    \n",
    "    # Extract encoded features\n",
    "    encoded_features = encoder.predict(X_images)\n",
    "    encoded_features_flat = encoded_features.reshape(subset_size, -1)\n",
    "    \n",
    "    # Standardize encoded features\n",
    "    encoded_scaled = scaler.fit_transform(encoded_features_flat)\n",
    "    \n",
    "    # Apply K-means on encoded features\n",
    "    print(\"Running K-means on Autoencoder features...\")\n",
    "    kmeans_ae = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    ae_labels = kmeans_ae.fit_predict(encoded_scaled)\n",
    "    \n",
    "    # Compute metrics\n",
    "    sil_score_ae = silhouette_score(encoded_scaled, ae_labels, sample_size=1000)\n",
    "    db_score_ae = davies_bouldin_score(encoded_scaled, ae_labels)\n",
    "    \n",
    "    # Decode centroids to image space\n",
    "    centroids_encoded = kmeans_ae.cluster_centers_.reshape(num_clusters, 7, 7, 16)\n",
    "    centroids_decoded = decoder.predict(centroids_encoded).reshape(num_clusters, 28, 28)\n",
    "    \n",
    "    # Store results\n",
    "    results['Autoencoder_KMeans'] = {\n",
    "        'labels': ae_labels,\n",
    "        'silhouette_score': sil_score_ae,\n",
    "        'davies_bouldin_score': db_score_ae,\n",
    "        'num_clusters': num_clusters,\n",
    "        'centroids': centroids_decoded,\n",
    "        'reconstruction_loss': autoencoder.evaluate(X_images, X_images, verbose=0)\n",
    "    }\n",
    "    print(f\"Autoencoder + K-means: {num_clusters} clusters, Silhouette: {sil_score_ae:.4f}, \"\n",
    "          f\"DB: {db_score_ae:.4f}, Reconstruction Loss: {results['Autoencoder_KMeans']['reconstruction_loss']:.4f}\")\n",
    "    \n",
    "    # Compare with other algorithms\n",
    "    algorithms = {\n",
    "        'KMeans': KMeans(n_clusters=num_clusters, random_state=42),\n",
    "        'GMM': GaussianMixture(n_components=num_clusters, random_state=42),\n",
    "        'HDBSCAN': hdbscan.HDBSCAN(min_cluster_size=50)\n",
    "    }\n",
    "    \n",
    "    for name, algo in algorithms.items():\n",
    "        print(f\"Running {name}...\")\n",
    "        try:\n",
    "            if name == 'GMM':\n",
    "                cluster_labels = algo.fit_predict(X_scaled)\n",
    "            else:\n",
    "                cluster_labels = algo.fit_predict(X_scaled)\n",
    "            \n",
    "            num_clusters_found = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "            \n",
    "            if num_clusters_found > 1:\n",
    "                sil_score = silhouette_score(X_scaled, cluster_labels, sample_size=1000)\n",
    "                db_score = davies_bouldin_score(X_scaled, cluster_labels)\n",
    "            else:\n",
    "                sil_score = np.nan\n",
    "                db_score = np.nan\n",
    "            \n",
    "            results[name] = {\n",
    "                'labels': cluster_labels,\n",
    "                'silhouette_score': sil_score,\n",
    "                'davies_bouldin_score': db_score,\n",
    "                'num_clusters': num_clusters_found,\n",
    "                'centroids': (algo.cluster_centers_ if name == 'KMeans' else \n",
    "                             algo.means_ if name == 'GMM' else None)\n",
    "            }\n",
    "            print(f\"{name}: {num_clusters_found} clusters, Silhouette: {sil_score:.4f}, DB: {db_score:.4f}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name}: {str(e)}\")\n",
    "            results[name] = {\n",
    "                'labels': None,\n",
    "                'silhouette_score': np.nan,\n",
    "                'davies_bouldin_score': np.nan,\n",
    "                'num_clusters': 0,\n",
    "                'centroids': None\n",
    "            }\n",
    "    \n",
    "    # Compare results\n",
    "    comparison = {\n",
    "        'Algorithm': [],\n",
    "        'Silhouette Score': [],\n",
    "        'Davies-Bouldin Score': [],\n",
    "        'Number of Clusters': []\n",
    "    }\n",
    "    for name, result in results.items():\n",
    "        comparison['Algorithm'].append(name)\n",
    "        comparison['Silhouette Score'].append(result['silhouette_score'])\n",
    "        comparison['Davies-Bouldin Score'].append(result['davies_bouldin_score'])\n",
    "        comparison['Number of Clusters'].append(result['num_clusters'])\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    print(\"\\nComparison of Clustering Algorithms:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Visualize centroids\n",
    "    for name in ['KMeans', 'GMM', 'Autoencoder_KMeans']:\n",
    "        if name in results and results[name]['labels'] is not None and results[name]['centroids'] is not None:\n",
    "            centroids = results[name]['centroids'].reshape(num_clusters, 28, 28)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            for i in range(num_clusters):\n",
    "                plt.subplot(2, 5, i + 1)\n",
    "                plt.imshow(centroids[i], cmap='gray')\n",
    "                plt.title(f\"Cluster {i}\")\n",
    "                plt.axis('off')\n",
    "            plt.suptitle(f\"{name} Cluster Centroids\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31caeb81",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset_path = \"../datasets/unlabelled_train_data_images.npy\"\n",
    "explore_and_visualize_digits(image_dataset_path, num_samples=25, subset_size=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eaf4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_dataset_path = \"../datasets/unlabelled_train_data_images.npy\"\n",
    "results = cluster_digits_with_autoencoder(image_dataset_path, num_clusters=10, subset_size=20000)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ccf08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import hdbscan\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import pandas as pd\n",
    "from scipy.ndimage import center_of_mass, rotate\n",
    "\n",
    "def preprocess_images_for_rotation(X_images):\n",
    "    X_aligned = np.zeros_like(X_images)\n",
    "    for i in range(X_images.shape[0]):\n",
    "        img = X_images[i].squeeze()\n",
    "        cy, cx = center_of_mass(img)\n",
    "        if np.isnan(cy) or np.isnan(cx):\n",
    "            X_aligned[i] = X_images[i]\n",
    "            continue\n",
    "        m = np.zeros((3, 3))\n",
    "        for y in range(img.shape[0]):\n",
    "            for x in range(img.shape[1]):\n",
    "                m[0, 0] += img[y, x]\n",
    "                m[1, 0] += (x - cx) * img[y, x]\n",
    "                m[0, 1] += (y - cy) * img[y, x]\n",
    "                m[1, 1] += (x - cx) * (y - cy) * img[y, x]\n",
    "                m[2, 0] += (x - cx)**2 * img[y, x]\n",
    "                m[0, 2] += (y - cy)**2 * img[y, x]\n",
    "        if m[0, 0] == 0:\n",
    "            X_aligned[i] = X_images[i]\n",
    "            continue\n",
    "        angle = 0.5 * np.arctan2(2 * m[1, 1], m[2, 0] - m[0, 2]) * 180 / np.pi\n",
    "        img_rotated = rotate(img, -angle, reshape=False, mode='nearest')\n",
    "        X_aligned[i] = img_rotated[np.newaxis, :, :] if X_images.shape[1] == 1 else img_rotated[:, :, np.newaxis]\n",
    "    return X_aligned\n",
    "\n",
    "def cluster_digits_improved(image_dataset_path, num_clusters=10, subset_size=20000, use_augmentation=True):\n",
    "    # Load and preprocess dataset\n",
    "    Xtrain_img = np.load(image_dataset_path)\n",
    "    print(\"Dataset shape:\", Xtrain_img.shape)\n",
    "    print(f\"Pixel value range: [{Xtrain_img.min():.2f}, {Xtrain_img.max():.2f}]\")\n",
    "    \n",
    "    indices = np.random.choice(Xtrain_img.shape[0], subset_size, replace=False)\n",
    "    X_subset = Xtrain_img[indices]\n",
    "    print(\"Subset shape:\", X_subset.shape)\n",
    "    \n",
    "    print(\"Aligning images...\")\n",
    "    X_aligned = preprocess_images_for_rotation(X_subset)\n",
    "    X_flat = X_aligned.reshape(subset_size, -1)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X_flat)\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    # Autoencoder + K-means\n",
    "    print(\"Training Convolutional Autoencoder...\")\n",
    "    X_images = X_aligned.reshape(-1, 28, 28, 1)\n",
    "    \n",
    "    # Build autoencoder\n",
    "    input_img = layers.Input(shape=(28, 28, 1))\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(input_img)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(x)\n",
    "    encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "    \n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(encoded)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
    "    decoded = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    \n",
    "    autoencoder = models.Model(input_img, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    # Define encoder\n",
    "    encoder = models.Model(input_img, encoded)\n",
    "    \n",
    "    # Define decoder explicitly\n",
    "    encoded_input = layers.Input(shape=(4, 4, 16))\n",
    "    x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_input)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(32, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D((2, 2))(x)\n",
    "    x = layers.Cropping2D(cropping=((2, 2), (2, 2)))(x)\n",
    "    x = layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "    decoder = models.Model(encoded_input, x)\n",
    "    \n",
    "    # Copy weights from autoencoder to decoder\n",
    "    for l1, l2 in zip(decoder.layers[1:], autoencoder.layers[7:]):\n",
    "        l1.set_weights(l2.get_weights())\n",
    "    \n",
    "    # Train autoencoder\n",
    "    if use_augmentation:\n",
    "        datagen = ImageDataGenerator(rotation_range=30, fill_mode='nearest')\n",
    "        datagen.fit(X_images)\n",
    "        autoencoder.fit(datagen.flow(X_images, X_images, batch_size=128), \n",
    "                        steps_per_epoch=len(X_images) // 128, epochs=20, verbose=1)\n",
    "    else:\n",
    "        autoencoder.fit(X_images, X_images, epochs=20, batch_size=128, verbose=1)\n",
    "    \n",
    "    # Extract encoded features\n",
    "    encoded_features = encoder.predict(X_images)\n",
    "    encoded_features_flat = encoded_features.reshape(subset_size, -1)\n",
    "    encoded_scaled = scaler.fit_transform(encoded_features_flat)\n",
    "    \n",
    "    # K-means on encoded features\n",
    "    print(\"Running K-means on Autoencoder features...\")\n",
    "    kmeans_ae = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "    ae_labels = kmeans_ae.fit_predict(encoded_scaled)\n",
    "    \n",
    "    sil_score_ae = silhouette_score(encoded_scaled, ae_labels, sample_size=1000)\n",
    "    db_score_ae = davies_bouldin_score(encoded_scaled, ae_labels)\n",
    "    \n",
    "    # Decode centroids\n",
    "    centroids_encoded = kmeans_ae.cluster_centers_.reshape(num_clusters, 4, 4, 16)\n",
    "    centroids_decoded = decoder.predict(centroids_encoded).reshape(num_clusters, 28, 28)\n",
    "    \n",
    "    results['Autoencoder_KMeans'] = {\n",
    "        'labels': ae_labels,\n",
    "        'silhouette_score': sil_score_ae,\n",
    "        'davies_bouldin_score': db_score_ae,\n",
    "        'num_clusters': num_clusters,\n",
    "        'centroids': centroids_decoded,\n",
    "        'reconstruction_loss': autoencoder.evaluate(X_images, X_images, verbose=0)\n",
    "    }\n",
    "    print(f\"Autoencoder + K-means: {num_clusters} clusters, Silhouette: {sil_score_ae:.4f}, \"\n",
    "          f\"DB: {db_score_ae:.4f}, Reconstruction Loss: {results['Autoencoder_KMeans']['reconstruction_loss']:.4f}\")\n",
    "    \n",
    "    # Other algorithms\n",
    "    algorithms = {\n",
    "        'KMeans': KMeans(n_clusters=num_clusters, random_state=42),\n",
    "        'GMM': GaussianMixture(n_components=num_clusters, random_state=42),\n",
    "        'HDBSCAN': hdbscan.HDBSCAN(min_cluster_size=20)\n",
    "    }\n",
    "    \n",
    "    for name, algo in algorithms.items():\n",
    "        print(f\"Running {name}...\")\n",
    "        try:\n",
    "            if name == 'GMM':\n",
    "                cluster_labels = algo.fit_predict(X_scaled)\n",
    "            else:\n",
    "                cluster_labels = algo.fit_predict(X_scaled)\n",
    "            \n",
    "            num_clusters_found = len(set(cluster_labels)) - (1 if -1 in cluster_labels else 0)\n",
    "            sil_score = silhouette_score(X_scaled, cluster_labels, sample_size=1000) if num_clusters_found > 1 else np.nan\n",
    "            db_score = davies_bouldin_score(X_scaled, cluster_labels) if num_clusters_found > 1 else np.nan\n",
    "            \n",
    "            results[name] = {\n",
    "                'labels': cluster_labels,\n",
    "                'silhouette_score': sil_score,\n",
    "                'davies_bouldin_score': db_score,\n",
    "                'num_clusters': num_clusters_found,\n",
    "                'centroids': (algo.cluster_centers_ if name == 'KMeans' else algo.means_ if name == 'GMM' else None)\n",
    "            }\n",
    "            print(f\"{name}: {num_clusters_found} clusters, Silhouette: {sil_score:.4f}, DB: {db_score:.4f}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error in {name}: {str(e)}\")\n",
    "            results[name] = {'labels': None, 'silhouette_score': np.nan, 'davies_bouldin_score': np.nan, 'num_clusters': 0, 'centroids': None}\n",
    "    \n",
    "    # Comparison\n",
    "    comparison = {\n",
    "        'Algorithm': [],\n",
    "        'Silhouette Score': [],\n",
    "        'Davies-Bouldin Score': [],\n",
    "        'Number of Clusters': []\n",
    "    }\n",
    "    for name, result in results.items():\n",
    "        comparison['Algorithm'].append(name)\n",
    "        comparison['Silhouette Score'].append(result['silhouette_score'])\n",
    "        comparison['Davies-Bouldin Score'].append(result['davies_bouldin_score'])\n",
    "        comparison['Number of Clusters'].append(result['num_clusters'])\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison)\n",
    "    print(\"\\nComparison of Clustering Algorithms:\")\n",
    "    print(comparison_df)\n",
    "    \n",
    "    # Visualize centroids\n",
    "    for name in ['KMeans', 'GMM', 'Autoencoder_KMeans']:\n",
    "        if name in results and results[name]['centroids'] is not None:\n",
    "            centroids = results[name]['centroids'].reshape(num_clusters, 28, 28)\n",
    "            plt.figure(figsize=(10, 4))\n",
    "            for i in range(num_clusters):\n",
    "                plt.subplot(2, 5, i + 1)\n",
    "                plt.imshow(centroids[i], cmap='gray')\n",
    "                plt.title(f\"Cluster {i}\")\n",
    "                plt.axis('off')\n",
    "            plt.suptitle(f\"{name} Cluster Centroids\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    image_dataset_path = \"../datasets/unlabelled_train_data_images.npy\"\n",
    "    results = cluster_digits_improved(image_dataset_path, num_clusters=10, subset_size=20000, use_augmentation=True)\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
