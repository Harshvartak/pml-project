{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d62af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Load and Preprocess Data\n",
    "def load_and_preprocess_data(file_path):\n",
    "    \"\"\"\n",
    "    Load and preprocess the MNIST-based dataset.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the .npy file.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Preprocessed data of shape (n_samples, 784).\n",
    "    \"\"\"\n",
    "    X = np.load(file_path)\n",
    "    # Reshape from (60000, 1, 28, 28) to (60000, 784)\n",
    "    X = X.reshape(X.shape[0], -1)\n",
    "    # Normalize to [0, 1] assuming pixel values are 0-255\n",
    "    X = X / 255.0\n",
    "    return X\n",
    "\n",
    "# 2. Dimensionality Reduction with PCA\n",
    "def apply_pca(X, variance_ratio=0.95):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce dimensionality while retaining specified variance.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        variance_ratio (float): Fraction of variance to retain (default: 0.95).\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Reduced data.\n",
    "    \"\"\"\n",
    "    pca = PCA()\n",
    "    pca.fit(X)\n",
    "    cumsum = np.cumsum(pca.explained_variance_ratio_)\n",
    "    n_components = np.argmax(cumsum >= variance_ratio) + 1\n",
    "    print(f'Number of components to explain {variance_ratio*100}% variance: {n_components}')\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X)\n",
    "    return X_pca\n",
    "\n",
    "# 3. Clustering\n",
    "def perform_clustering(X, algorithm='kmeans', n_clusters=10, random_state=42, **kwargs):\n",
    "    \"\"\"\n",
    "    Perform clustering on the data using the specified algorithm.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        algorithm (str): Clustering algorithm ('kmeans', 'minibatch_kmeans', 'agglomerative').\n",
    "        n_clusters (int): Number of clusters (default: 10).\n",
    "        random_state (int): Random seed for reproducibility (default: 42).\n",
    "        **kwargs: Additional arguments for the clustering algorithm.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Cluster labels.\n",
    "    \"\"\"\n",
    "    if algorithm == 'kmeans':\n",
    "        clusterer = KMeans(n_clusters=n_clusters, random_state=random_state, **kwargs)\n",
    "    elif algorithm == 'minibatch_kmeans':\n",
    "        clusterer = MiniBatchKMeans(n_clusters=n_clusters, random_state=random_state, **kwargs)\n",
    "    elif algorithm == 'agglomerative':\n",
    "        clusterer = AgglomerativeClustering(n_clusters=n_clusters, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported algorithm: {algorithm}\")\n",
    "    labels = clusterer.fit_predict(X)\n",
    "    return labels\n",
    "\n",
    "# 4. Evaluation\n",
    "def evaluate_clustering(X, labels):\n",
    "    \"\"\"\n",
    "    Evaluate clustering performance using internal metrics.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        labels (np.ndarray): Cluster labels.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (silhouette_score, davies_bouldin_score, calinski_harabasz_score).\n",
    "    \"\"\"\n",
    "    if len(set(labels)) > 1:\n",
    "        sil_score = silhouette_score(X, labels)\n",
    "        db_score = davies_bouldin_score(X, labels)\n",
    "        ch_score = calinski_harabasz_score(X, labels)\n",
    "        return sil_score, db_score, ch_score\n",
    "    else:\n",
    "        return -1, -1, -1\n",
    "\n",
    "# 5. Visualization with t-SNE\n",
    "def visualize_clusters(X, labels, algorithm_name, sample_size=1000):\n",
    "    \"\"\"\n",
    "    Visualize clusters using t-SNE.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Input data of shape (n_samples, n_features).\n",
    "        labels (np.ndarray): Cluster labels.\n",
    "        algorithm_name (str): Name of the clustering algorithm for the plot title.\n",
    "        sample_size (int): Number of samples to visualize (default: 1000).\n",
    "    \"\"\"\n",
    "    # Subsample for faster t-SNE computation\n",
    "    if X.shape[0] > sample_size:\n",
    "        idx = np.random.choice(X.shape[0], sample_size, replace=False)\n",
    "        X_sample = X[idx]\n",
    "        labels_sample = labels[idx]\n",
    "    else:\n",
    "        X_sample = X\n",
    "        labels_sample = labels\n",
    "    \n",
    "    tsne = TSNE(n_components=2, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_sample)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    unique_labels = np.unique(labels_sample)\n",
    "    for i in unique_labels:\n",
    "        plt.scatter(X_tsne[labels_sample == i, 0], X_tsne[labels_sample == i, 1], \n",
    "                   label=f'Cluster {i}', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.title(f't-SNE Visualization of {algorithm_name} Clusters')\n",
    "    plt.xlabel('t-SNE Component 1')\n",
    "    plt.ylabel('t-SNE Component 2')\n",
    "    plt.show()\n",
    "\n",
    "# Main Execution\n",
    "if __name__ == \"__main__\":\n",
    "    # File path\n",
    "    file_path = '../datasets/unlabelled_train_data_images.npy'\n",
    "    \n",
    "    # Load and preprocess data\n",
    "    X_train = load_and_preprocess_data(file_path)\n",
    "    print(f\"X_train shape after preprocessing: {X_train.shape}\")\n",
    "    \n",
    "    # Apply PCA\n",
    "    X_pca = apply_pca(X_train, variance_ratio=0.95)\n",
    "    print(f\"X_pca shape: {X_pca.shape}\")\n",
    "    \n",
    "    # List of clustering algorithms to try\n",
    "    algorithms = [\n",
    "        ('kmeans', 'K-Means'),\n",
    "        ('minibatch_kmeans', 'MiniBatch K-Means'),\n",
    "        ('agglomerative', 'Agglomerative Clustering')\n",
    "    ]\n",
    "    \n",
    "    # Perform clustering and evaluate\n",
    "    for algo_key, algo_name in algorithms:\n",
    "        print(f\"\\nPerforming {algo_name} clustering...\")\n",
    "        labels = perform_clustering(X_pca, algorithm=algo_key, n_clusters=10)\n",
    "        sil_score, db_score, ch_score = evaluate_clustering(X_pca, labels)\n",
    "        print(f\"{algo_name} - Silhouette: {sil_score:.4f}, DB: {db_score:.4f}, CH: {ch_score:.4f}\")\n",
    "        visualize_clusters(X_pca, labels, algo_name)\n",
    "        \n",
    "        # Assign labels for the last algorithm (e.g., MiniBatch K-Means) as Y_train\n",
    "        if algo_key == 'minibatch_kmeans':\n",
    "            Y_train = labels\n",
    "    \n",
    "    print(f\"\\nX_train shape: {X_train.shape}\")\n",
    "    print(f\"Y_train shape: {Y_train.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
