{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef4c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dd8d3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "image_dataset = '../datasets/unlabelled_train_data_images.npy'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f169cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class MNISTPreprocessor:\n",
    "    \"\"\"\n",
    "    A class to preprocess and label an unlabeled MNIST-like dataset using PyTorch and scikit-learn,\n",
    "    with multiple clustering algorithms, t-SNE visualization, and comprehensive metrics for analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path, n_clusters=10, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the preprocessor.\n",
    "        \n",
    "        Args:\n",
    "            data_path (str): Path to the .npy file containing the unlabeled images.\n",
    "            n_clusters (int): Number of clusters for clustering algorithms (default: 10 for digits 0-9).\n",
    "            random_state (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.data_path = data_path\n",
    "        self.n_clusters = n_clusters\n",
    "        self.random_state = random_state\n",
    "        self.images = None\n",
    "        self.preprocessed_images = None\n",
    "        self.labels = None\n",
    "        self.metrics = {}  # Store metrics for analysis\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"\n",
    "        Load the dataset from the .npy file and compute initial data metrics.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Loaded images with shape (n_samples, 1, 28, 28).\n",
    "        \"\"\"\n",
    "        self.images = np.load(self.data_path)\n",
    "        print(f\"Loaded data shape: {self.images.shape}\")\n",
    "        \n",
    "        # Validate data\n",
    "        if self.images.shape[1:] != (1, 28, 28):\n",
    "            raise ValueError(\"Expected images with shape (n_samples, 1, 28, 28)\")\n",
    "        if np.any(np.isnan(self.images)) or np.any(np.isinf(self.images)):\n",
    "            raise ValueError(\"Data contains NaN or infinite values\")\n",
    "        \n",
    "        # Compute initial pixel statistics\n",
    "        self.metrics['raw_pixel_stats'] = {\n",
    "            'mean': np.mean(self.images),\n",
    "            'std': np.std(self.images),\n",
    "            'min': np.min(self.images),\n",
    "            'max': np.max(self.images)\n",
    "        }\n",
    "        print(\"Raw pixel statistics:\", self.metrics['raw_pixel_stats'])\n",
    "        \n",
    "        return self.images\n",
    "    \n",
    "    def preprocess_images(self):\n",
    "        \"\"\"\n",
    "        Preprocess the images: normalize and convert to PyTorch tensors, compute pixel statistics.\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Preprocessed images with shape (n_samples, 1, 28, 28).\n",
    "        \"\"\"\n",
    "        if self.images is None:\n",
    "            self.load_data()\n",
    "        \n",
    "        # Normalize to [0, 1]\n",
    "        self.preprocessed_images = self.images.astype(np.float32) / 255.0\n",
    "        print(f\"Preprocessed data shape: {self.preprocessed_images.shape}, range: [{self.preprocessed_images.min()}, {self.preprocessed_images.max()}]\")\n",
    "        \n",
    "        # Compute preprocessed pixel statistics\n",
    "        self.metrics['preprocessed_pixel_stats'] = {\n",
    "            'mean': np.mean(self.preprocessed_images),\n",
    "            'std': np.std(self.preprocessed_images),\n",
    "            'min': np.min(self.preprocessed_images),\n",
    "            'max': np.max(self.preprocessed_images)\n",
    "        }\n",
    "        print(\"Preprocessed pixel statistics:\", self.metrics['preprocessed_pixel_stats'])\n",
    "        \n",
    "        # Convert to PyTorch tensor\n",
    "        tensor_images = torch.from_numpy(self.preprocessed_images)\n",
    "        return tensor_images\n",
    "    \n",
    "    def flatten_for_clustering(self, apply_pca=False, n_components=50):\n",
    "        \"\"\"\n",
    "        Flatten images for clustering and optionally apply PCA, compute PCA metrics.\n",
    "        \n",
    "        Args:\n",
    "            apply_pca (bool): Whether to apply PCA for dimensionality reduction.\n",
    "            n_components (int): Number of PCA components if apply_pca is True.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Flattened or PCA-transformed images with shape (n_samples, n_features).\n",
    "        \"\"\"\n",
    "        if self.preprocessed_images is None:\n",
    "            self.preprocess_images()\n",
    "        \n",
    "        # Flatten images: (n_samples, 1, 28, 28) -> (n_samples, 784)\n",
    "        flattened = self.preprocessed_images.reshape(self.preprocessed_images.shape[0], -1)\n",
    "        print(f\"Flattened data shape: {flattened.shape}\")\n",
    "        \n",
    "        if apply_pca:\n",
    "            pca = PCA(n_components=n_components, random_state=self.random_state)\n",
    "            flattened = pca.fit_transform(flattened)\n",
    "            print(f\"PCA-transformed data shape: {flattened.shape}\")\n",
    "            \n",
    "            # Store PCA metrics\n",
    "            self.metrics['pca_metrics'] = {\n",
    "                'n_components': n_components,\n",
    "                'explained_variance_ratio': np.sum(pca.explained_variance_ratio_),\n",
    "                'individual_explained_variance': pca.explained_variance_ratio_.tolist()\n",
    "            }\n",
    "            print(f\"PCA explained variance ratio: {self.metrics['pca_metrics']['explained_variance_ratio']:.4f}\")\n",
    "        \n",
    "        return flattened\n",
    "    \n",
    "    def compute_clustering_metrics(self, flattened_images, labels, algorithm, subset_ratio=0.1):\n",
    "        \"\"\"\n",
    "        Compute clustering quality metrics: silhouette score, Davies-Bouldin index, cluster sizes,\n",
    "        and algorithm-specific metrics.\n",
    "        \n",
    "        Args:\n",
    "            flattened_images (numpy.ndarray): Flattened or PCA-transformed images.\n",
    "            labels (numpy.ndarray): Cluster labels.\n",
    "            algorithm (str): Name of the clustering algorithm ('kmeans', 'agglomerative', 'dbscan', 'gmm').\n",
    "            subset_ratio (float): Fraction of data to use for silhouette and DB scores (default: 0.1).\n",
    "        \n",
    "        Returns:\n",
    "            dict: Clustering metrics.\n",
    "        \"\"\"\n",
    "        # Use a subset for silhouette and DB scores\n",
    "        n_subset = int(flattened_images.shape[0] * subset_ratio)\n",
    "        subset_indices = np.random.choice(flattened_images.shape[0], size=n_subset, replace=False)\n",
    "        subset_images = flattened_images[subset_indices]\n",
    "        subset_labels = labels[subset_indices]\n",
    "        \n",
    "        # Initialize metrics\n",
    "        clustering_metrics = {}\n",
    "        \n",
    "        # Cluster sizes\n",
    "        unique_labels = np.unique(labels)\n",
    "        if -1 in unique_labels:  # Handle DBSCAN noise\n",
    "            cluster_sizes = np.bincount(labels[labels != -1], minlength=len(unique_labels) - 1)\n",
    "            clustering_metrics['n_noise_points'] = np.sum(labels == -1)\n",
    "        else:\n",
    "            cluster_sizes = np.bincount(labels, minlength=self.n_clusters)\n",
    "        clustering_metrics['cluster_sizes'] = cluster_sizes.tolist()\n",
    "        clustering_metrics['n_clusters'] = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "        \n",
    "        # Silhouette score (requires at least 2 clusters)\n",
    "        if clustering_metrics['n_clusters'] >= 2:\n",
    "            clustering_metrics['silhouette_score'] = silhouette_score(subset_images, subset_labels)\n",
    "        else:\n",
    "            clustering_metrics['silhouette_score'] = None\n",
    "            print(\"Silhouette score not computed (fewer than 2 clusters)\")\n",
    "        \n",
    "        # Davies-Bouldin index (requires at least 2 clusters)\n",
    "        if clustering_metrics['n_clusters'] >= 2:\n",
    "            clustering_metrics['davies_bouldin_score'] = davies_bouldin_score(subset_images, subset_labels)\n",
    "        else:\n",
    "            clustering_metrics['davies_bouldin_score'] = None\n",
    "            print(\"Davies-Bouldin index not computed (fewer than 2 clusters)\")\n",
    "        \n",
    "        # Algorithm-specific metrics\n",
    "        if algorithm == 'kmeans' or algorithm == 'agglomerative':\n",
    "            # Compute inertia (within-cluster sum of squares)\n",
    "            inertia = 0\n",
    "            for label in unique_labels:\n",
    "                if label == -1:  # Skip noise for DBSCAN\n",
    "                    continue\n",
    "                cluster_points = flattened_images[labels == label]\n",
    "                centroid = np.mean(cluster_points, axis=0)\n",
    "                inertia += np.sum((cluster_points - centroid) ** 2)\n",
    "            clustering_metrics['inertia'] = inertia\n",
    "        elif algorithm == 'gmm':\n",
    "            # Log-likelihood and BIC are stored during clustering\n",
    "            clustering_metrics['log_likelihood'] = self.metrics.get('gmm_log_likelihood', None)\n",
    "            clustering_metrics['bic'] = self.metrics.get('gmm_bic', None)\n",
    "        \n",
    "        print(f\"Clustering metrics for {algorithm} (subset {subset_ratio*100}%):\")\n",
    "        if clustering_metrics['silhouette_score'] is not None:\n",
    "            print(f\"  Silhouette Score: {clustering_metrics['silhouette_score']:.4f}\")\n",
    "        if clustering_metrics['davies_bouldin_score'] is not None:\n",
    "            print(f\"  Davies-Bouldin Index: {clustering_metrics['davies_bouldin_score']:.4f}\")\n",
    "        print(f\"  Number of Clusters: {clustering_metrics['n_clusters']}\")\n",
    "        print(f\"  Cluster Sizes: {clustering_metrics['cluster_sizes']}\")\n",
    "        if 'inertia' in clustering_metrics:\n",
    "            print(f\"  Inertia: {clustering_metrics['inertia']:.2f}\")\n",
    "        if 'n_noise_points' in clustering_metrics:\n",
    "            print(f\"  Noise Points: {clustering_metrics['n_noise_points']}\")\n",
    "        if 'log_likelihood' in clustering_metrics:\n",
    "            print(f\"  Log-Likelihood: {clustering_metrics['log_likelihood']:.2f}\")\n",
    "        if 'bic' in clustering_metrics:\n",
    "            print(f\"  BIC: {clustering_metrics['bic']:.2f}\")\n",
    "        \n",
    "        return clustering_metrics\n",
    "    \n",
    "    def label_with_clustering(self, algorithm='kmeans', apply_pca=False, n_components=50, dbscan_eps=0.5, dbscan_min_samples=5):\n",
    "        \"\"\"\n",
    "        Label the dataset using the specified clustering algorithm and compute metrics.\n",
    "        \n",
    "        Args:\n",
    "            algorithm (str): Clustering algorithm ('kmeans', 'agglomerative', 'dbscan', 'gmm').\n",
    "            apply_pca (bool): Whether to apply PCA before clustering.\n",
    "            n_components (int): Number of PCA components if apply_pca is True.\n",
    "            dbscan_eps (float): DBSCAN epsilon parameter (distance threshold).\n",
    "            dbscan_min_samples (int): DBSCAN minimum samples for a core point.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Cluster labels with shape (n_samples,).\n",
    "        \"\"\"\n",
    "        flattened_images = self.flatten_for_clustering(apply_pca, n_components)\n",
    "        \n",
    "        # Apply clustering\n",
    "        if algorithm == 'kmeans':\n",
    "            model = KMeans(n_clusters=self.n_clusters, random_state=self.random_state, n_init=10)\n",
    "            self.labels = model.fit_predict(flattened_images)\n",
    "            self.metrics['inertia'] = model.inertia_\n",
    "            print(f\"Generated {self.n_clusters} clusters with K-Means\")\n",
    "            print(f\"K-Means Inertia: {self.metrics['inertia']:.2f}\")\n",
    "        \n",
    "        elif algorithm == 'agglomerative':\n",
    "            model = AgglomerativeClustering(n_clusters=self.n_clusters, linkage='ward')\n",
    "            self.labels = model.fit_predict(flattened_images)\n",
    "            print(f\"Generated {self.n_clusters} clusters with Agglomerative Clustering\")\n",
    "        \n",
    "        elif algorithm == 'dbscan':\n",
    "            model = DBSCAN(eps=dbscan_eps, min_samples=dbscan_min_samples, n_jobs=-1)\n",
    "            self.labels = model.fit_predict(flattened_images)\n",
    "            n_clusters = len(np.unique(self.labels)) - (1 if -1 in self.labels else 0)\n",
    "            print(f\"Generated {n_clusters} clusters with DBSCAN (eps={dbscan_eps}, min_samples={dbscan_min_samples})\")\n",
    "        \n",
    "        elif algorithm == 'gmm':\n",
    "            model = GaussianMixture(n_components=self.n_clusters, covariance_type='full', random_state=self.random_state)\n",
    "            self.labels = model.fit_predict(flattened_images)\n",
    "            self.metrics['gmm_log_likelihood'] = model.score(flattened_images) * flattened_images.shape[0]  # Total log-likelihood\n",
    "            self.metrics['gmm_bic'] = model.bic(flattened_images)\n",
    "            print(f\"Generated {self.n_clusters} clusters with GMM\")\n",
    "            print(f\"GMM Log-Likelihood: {self.metrics['gmm_log_likelihood']:.2f}\")\n",
    "            print(f\"GMM BIC: {self.metrics['gmm_bic']:.2f}\")\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(\"Unsupported algorithm. Choose 'kmeans', 'agglomerative', 'dbscan', or 'gmm'.\")\n",
    "        \n",
    "        # Compute clustering metrics\n",
    "        self.metrics[f'{algorithm}_clustering_metrics'] = self.compute_clustering_metrics(flattened_images, self.labels, algorithm)\n",
    "        \n",
    "        return self.labels\n",
    "    \n",
    "    def visualize_clusters(self, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize a few images from each cluster.\n",
    "        \n",
    "        Args:\n",
    "            save_path (str, optional): Path to save the visualization plot.\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Run label_with_clustering() first to generate labels\")\n",
    "        \n",
    "        unique_labels = np.unique(self.labels)\n",
    "        n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "        fig, axes = plt.subplots(n_clusters, 5, figsize=(15, 2 * n_clusters))\n",
    "        axes = np.array(axes).reshape(n_clusters, 5) if n_clusters > 1 else np.array([axes])\n",
    "        \n",
    "        for i, cluster in enumerate(unique_labels):\n",
    "            if cluster == -1:  # Skip noise for DBSCAN\n",
    "                continue\n",
    "            cluster_indices = np.where(self.labels == cluster)[0]\n",
    "            selected_indices = np.random.choice(cluster_indices, size=min(5, len(cluster_indices)), replace=False)\n",
    "            for j, idx in enumerate(selected_indices):\n",
    "                axes[i, j].imshow(self.preprocessed_images[idx, 0], cmap='gray')\n",
    "                axes[i, j].axis('off')\n",
    "                if j == 0:\n",
    "                    axes[i, j].set_title(f\"Cluster {cluster}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Cluster visualization saved to {save_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    def visualize_tsne(self, subset_ratio=0.2, apply_pca=True, n_components=50, save_path=None):\n",
    "        \"\"\"\n",
    "        Apply t-SNE to visualize 20% of the dataset in 2D, colored by cluster labels.\n",
    "        \n",
    "        Args:\n",
    "            subset_ratio (float): Fraction of the dataset to use (default: 0.2 for 20%).\n",
    "            apply_pca (bool): Whether to apply PCA before t-SNE.\n",
    "            n_components (int): Number of PCA components if apply_pca is True.\n",
    "            save_path (str, optional): Path to save the t-SNE visualization plot.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: 2D t-SNE embeddings.\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            raise ValueError(\"Run label_with_clustering() first to generate labels\")\n",
    "        \n",
    "        n_samples = int(self.preprocessed_images.shape[0] * subset_ratio)\n",
    "        indices = np.random.choice(self.preprocessed_images.shape[0], size=n_samples, replace=False)\n",
    "        subset_images = self.preprocessed_images[indices]\n",
    "        subset_labels = self.labels[indices]\n",
    "        print(f\"Selected {n_samples} samples for t-SNE visualization\")\n",
    "        \n",
    "        flattened = subset_images.reshape(n_samples, -1)\n",
    "        if apply_pca:\n",
    "            pca = PCA(n_components=n_components, random_state=self.random_state)\n",
    "            flattened = pca.fit_transform(flattened)\n",
    "            self.metrics['tsne_pca_metrics'] = {\n",
    "                'n_components': n_components,\n",
    "                'explained_variance_ratio': np.sum(pca.explained_variance_ratio_)\n",
    "            }\n",
    "            print(f\"t-SNE PCA explained variance ratio: {self.metrics['tsne_pca_metrics']['explained_variance_ratio']:.4f}\")\n",
    "        \n",
    "        tsne = TSNE(n_components=2, random_state=self.random_state, n_jobs=-1)\n",
    "        tsne_embeddings = tsne.fit_transform(flattened)\n",
    "        print(f\"t-SNE embeddings shape: {tsne_embeddings.shape}\")\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        scatter = plt.scatter(tsne_embeddings[:, 0], tsne_embeddings[:, 1], c=subset_labels, cmap='tab10', alpha=0.6)\n",
    "        plt.colorbar(scatter, label='Cluster Label')\n",
    "        plt.title('t-SNE Visualization of 20% MNIST Dataset')\n",
    "        plt.xlabel('t-SNE Component 1')\n",
    "        plt.ylabel('t-SNE Component 2')\n",
    "        \n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"t-SNE visualization saved to {save_path}\")\n",
    "        plt.show()\n",
    "        \n",
    "        return tsne_embeddings\n",
    "    \n",
    "    def save_labeled_dataset(self, output_path):\n",
    "        \"\"\"\n",
    "        Save the preprocessed images, labels, and metrics to a .npz file.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str): Path to save the .npz file.\n",
    "        \"\"\"\n",
    "        if self.preprocessed_images is None or self.labels is None:\n",
    "            raise ValueError(\"Preprocess images and generate labels first\")\n",
    "        \n",
    "        np.savez(output_path, images=self.preprocessed_images, labels=self.labels)\n",
    "        print(f\"Labeled dataset saved to {output_path}\")\n",
    "    \n",
    "    def get_metrics_summary(self):\n",
    "        \"\"\"\n",
    "        Return a summary of all computed metrics.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Dictionary containing all metrics.\n",
    "        \"\"\"\n",
    "        return self.metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d06ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def processImages():\n",
    "    data_path = '../datasets/unlabelled_train_data_images.npy'\n",
    "    output_path = '../datasets/labeled_train_data.npz'\n",
    "    cluster_vis_path = '../datasets/cluster_visualization.png'\n",
    "    tsne_vis_path = '../datasets/tsne_visualization.png'\n",
    "    \n",
    "    preprocessor = MNISTPreprocessor(data_path, n_clusters=10, random_state=42)\n",
    "    \n",
    "    # Load and preprocess\n",
    "    preprocessor.load_data()\n",
    "    tensor_images = preprocessor.preprocess_images()\n",
    "    \n",
    "    # Test multiple clustering algorithms\n",
    "    # algorithms = ['kmeans', 'agglomerative', 'dbscan', 'gmm']\n",
    "    algorithms = ['kmeans', 'agglomerative', 'gmm']\n",
    "    for algo in algorithms:\n",
    "        print(f\"\\nRunning {algo} clustering...\")\n",
    "        labels = preprocessor.label_with_clustering(\n",
    "            algorithm=algo,\n",
    "            apply_pca=True,\n",
    "            n_components=50,\n",
    "            dbscan_eps=0.5,  # Adjust based on experimentation\n",
    "            dbscan_min_samples=5\n",
    "        )\n",
    "        \n",
    "        # Visualize clusters\n",
    "        preprocessor.visualize_clusters(save_path=cluster_vis_path.replace('.png', f'_{algo}.png'))\n",
    "        \n",
    "        # Visualize t-SNE\n",
    "        preprocessor.visualize_tsne(\n",
    "            subset_ratio=0.2,\n",
    "            apply_pca=True,\n",
    "            n_components=50,\n",
    "            save_path=tsne_vis_path.replace('.png', f'_{algo}.png')\n",
    "        )\n",
    "        \n",
    "        # Print metrics summary for this algorithm\n",
    "        metrics_summary = preprocessor.get_metrics_summary()\n",
    "        print(f\"\\nMetrics Summary for {algo}:\")\n",
    "        for key, value in metrics_summary.items():\n",
    "            if algo in key or 'pixel_stats' in key or 'pca_metrics' in key:\n",
    "                print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Save the labeled dataset (using the last algorithm's labels)\n",
    "    preprocessor.save_labeled_dataset(output_path)\n",
    "    \n",
    "    # Verify saved data\n",
    "    with np.load(output_path) as data:\n",
    "        saved_images = data['images']\n",
    "        saved_labels = data['labels']\n",
    "        print(f\"Saved data: images shape {saved_images.shape}, labels shape {saved_labels.shape}\")\n",
    "\n",
    "processImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a9688c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "class ImageLabelModifier:\n",
    "    \"\"\"\n",
    "    A class to manually modify labels in a labeled MNIST-like dataset saved as a .npz file,\n",
    "    with support for iterative cluster-to-digit mapping updates.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, npz_path, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the label modifier.\n",
    "        \n",
    "        Args:\n",
    "            npz_path (str): Path to the .npz file containing images and labels.\n",
    "            random_state (int): Random seed for reproducibility in visualizations.\n",
    "        \"\"\"\n",
    "        self.npz_path = npz_path\n",
    "        self.random_state = random_state\n",
    "        self.images = None\n",
    "        self.labels = None\n",
    "        self.n_clusters = 10  # Assuming 10 digit classes\n",
    "        \n",
    "    def load_npz_data(self):\n",
    "        \"\"\"\n",
    "        Load images and labels from the .npz file.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: (images, labels) as numpy arrays.\n",
    "        \"\"\"\n",
    "        with np.load(self.npz_path) as data:\n",
    "            self.images = data['images']\n",
    "            self.labels = data['labels']\n",
    "        \n",
    "        print(f\"Loaded data: images shape {self.images.shape}, labels shape {self.labels.shape}\")\n",
    "        \n",
    "        # Validate data\n",
    "        if self.images.shape[1:] != (1, 28, 28):\n",
    "            raise ValueError(\"Expected images with shape (n_samples, 1, 28, 28)\")\n",
    "        if self.labels.shape != (self.images.shape[0],):\n",
    "            raise ValueError(\"Labels shape does not match images\")\n",
    "        if not np.all(np.isin(self.labels, np.arange(self.n_clusters))):\n",
    "            raise ValueError(\"Labels contain values outside expected range [0, 9]\")\n",
    "        \n",
    "        return self.images, self.labels\n",
    "    \n",
    "    def visualize_current_labels(self, samples_per_cluster=5, save_path=None):\n",
    "        \"\"\"\n",
    "        Visualize a few images from each cluster to inspect current labels.\n",
    "        \n",
    "        Args:\n",
    "            samples_per_cluster (int): Number of images to show per cluster.\n",
    "            save_path (str, optional): Path to save the visualization plot.\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            self.load_npz_data()\n",
    "        \n",
    "        np.random.seed(self.random_state)\n",
    "        fig, axes = plt.subplots(self.n_clusters, samples_per_cluster, figsize=(3 * samples_per_cluster, 2 * self.n_clusters))\n",
    "        \n",
    "        for cluster in range(self.n_clusters):\n",
    "            # Get indices of images in this cluster\n",
    "            cluster_indices = np.where(self.labels == cluster)[0]\n",
    "            # Select up to samples_per_cluster random images\n",
    "            selected_indices = np.random.choice(cluster_indices, size=min(samples_per_cluster, len(cluster_indices)), replace=False)\n",
    "            for i, idx in enumerate(selected_indices):\n",
    "                axes[cluster, i].imshow(self.images[idx, 0], cmap='gray')\n",
    "                axes[cluster, i].axis('off')\n",
    "                if i == 0:\n",
    "                    axes[cluster, i].set_title(f\"Cluster {cluster}\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        if save_path:\n",
    "            plt.savefig(save_path)\n",
    "            print(f\"Current labels visualization saved to {save_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    def map_cluster_to_digit(self, cluster_to_digit_map):\n",
    "        \"\"\"\n",
    "        Apply a manual mapping from cluster IDs to true digit labels.\n",
    "        \n",
    "        Args:\n",
    "            cluster_to_digit_map (dict): Mapping from cluster ID to digit (e.g., {0: 5, 1: 2, ...}).\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Updated labels with shape (n_samples,).\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            self.load_npz_data()\n",
    "        \n",
    "        # Validate mapping\n",
    "        if not all(k in range(self.n_clusters) for k in cluster_to_digit_map.keys()):\n",
    "            raise ValueError(\"Cluster IDs in mapping must be in range [0, 9]\")\n",
    "        if not all(v in range(10) for v in cluster_to_digit_map.values()):\n",
    "            raise ValueError(\"Digit values in mapping must be in range [0, 9]\")\n",
    "        if len(set(cluster_to_digit_map.values())) != len(cluster_to_digit_map):\n",
    "            raise ValueError(\"Mapping must assign unique digits to clusters\")\n",
    "        \n",
    "        # Create new labels array\n",
    "        new_labels = np.copy(self.labels)\n",
    "        for cluster, digit in cluster_to_digit_map.items():\n",
    "            new_labels[self.labels == cluster] = digit\n",
    "        \n",
    "        self.labels = new_labels\n",
    "        print(\"Applied cluster-to-digit mapping\")\n",
    "        return self.labels\n",
    "    \n",
    "    def get_user_mapping(self):\n",
    "        \"\"\"\n",
    "        Prompt the user to input a cluster-to-digit mapping.\n",
    "        \n",
    "        Returns:\n",
    "            dict: Mapping from cluster ID to digit, or None if user skips.\n",
    "        \"\"\"\n",
    "        print(\"\\nEnter cluster-to-digit mapping (e.g., '0:5, 1:2, ...') or press Enter to skip.\")\n",
    "        print(\"Format: 'cluster:digit' pairs separated by commas, covering all clusters 0-9.\")\n",
    "        user_input = input(\"Mapping: \").strip()\n",
    "        \n",
    "        if not user_input:\n",
    "            print(\"No mapping provided, keeping current labels\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Parse input (e.g., \"0:5, 1:2, 2:1, ...\")\n",
    "            mapping = {}\n",
    "            pairs = user_input.split(',')\n",
    "            for pair in pairs:\n",
    "                cluster, digit = map(int, pair.strip().split(':'))\n",
    "                if cluster not in range(self.n_clusters) or digit not in range(10):\n",
    "                    raise ValueError(\"Invalid cluster or digit value\")\n",
    "                mapping[cluster] = digit\n",
    "            \n",
    "            # Validate completeness and uniqueness\n",
    "            if len(mapping) != self.n_clusters:\n",
    "                raise ValueError(f\"Mapping must include all {self.n_clusters} clusters\")\n",
    "            if len(set(mapping.values())) != self.n_clusters:\n",
    "                raise ValueError(\"Each cluster must map to a unique digit\")\n",
    "            \n",
    "            return mapping\n",
    "        except Exception as e:\n",
    "            print(f\"Error parsing mapping: {e}. Please try again.\")\n",
    "            return None\n",
    "    \n",
    "    def iterative_update_labels(self, samples_per_cluster=5, vis_path_prefix='labels_visualization'):\n",
    "        \"\"\"\n",
    "        Iteratively visualize clusters, prompt for mapping, and update labels.\n",
    "        \n",
    "        Args:\n",
    "            samples_per_cluster (int): Number of images to show per cluster.\n",
    "            vis_path_prefix (str): Prefix for visualization save paths.\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Final updated labels.\n",
    "        \"\"\"\n",
    "        if self.labels is None:\n",
    "            self.load_npz_data()\n",
    "        \n",
    "        iteration = 0\n",
    "        while True:\n",
    "            # Visualize current labels\n",
    "            vis_path = f\"../datasets/{vis_path_prefix}_iter{iteration}.png\"\n",
    "            self.visualize_current_labels(samples_per_cluster=samples_per_cluster, save_path=vis_path)\n",
    "            \n",
    "            # Get user mapping\n",
    "            mapping = self.get_user_mapping()\n",
    "            if mapping:\n",
    "                # Apply mapping\n",
    "                self.map_cluster_to_digit(mapping)\n",
    "            else:\n",
    "                print(\"No changes made to labels\")\n",
    "            \n",
    "            # Ask if user wants to continue\n",
    "            response = input(\"\\nDo you want to refine the mapping further? (yes/no): \").strip().lower()\n",
    "            if response != 'yes':\n",
    "                break\n",
    "            \n",
    "            iteration += 1\n",
    "        \n",
    "        # Final visualization\n",
    "        final_vis_path = f\"../datasets/{vis_path_prefix}_final.png\"\n",
    "        self.visualize_current_labels(samples_per_cluster=samples_per_cluster, save_path=final_vis_path)\n",
    "        print(\"Final labels visualization completed\")\n",
    "        return self.labels\n",
    "    \n",
    "    def save_updated_dataset(self, output_path):\n",
    "        \"\"\"\n",
    "        Save the images and updated labels to a new .npz file.\n",
    "        \n",
    "        Args:\n",
    "            output_path (str): Path to save the updated .npz file.\n",
    "        \"\"\"\n",
    "        if self.images is None or self.labels is None:\n",
    "            raise ValueError(\"Load data and modify labels first\")\n",
    "        \n",
    "        np.savez(output_path, images=self.images, labels=self.labels)\n",
    "        print(f\"Updated dataset saved to {output_path}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663646c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def processImages():\n",
    "#     data_path = '../datasets/unlabelled_train_data_images.npy'\n",
    "#     output_path = '../datasets/labeled_train_data.npz'\n",
    "#     cluster_vis_path = '../datasets/cluster_visualization.png'\n",
    "#     tsne_vis_path = '../datasets/tsne_visualization.png'\n",
    "    \n",
    "#     preprocessor = MNISTPreprocessor(data_path, n_clusters=10, random_state=42)\n",
    "    \n",
    "#     # Load and preprocess\n",
    "#     preprocessor.load_data()\n",
    "#     tensor_images = preprocessor.preprocess_images()\n",
    "    \n",
    "#     # Label using K-Means with PCA\n",
    "#     labels = preprocessor.label_with_kmeans(apply_pca=True, n_components=50)\n",
    "    \n",
    "#     # Visualize clusters\n",
    "#     preprocessor.visualize_clusters(save_path=cluster_vis_path)\n",
    "    \n",
    "#     # Visualize t-SNE on 20% of the data\n",
    "#     tsne_embeddings = preprocessor.visualize_tsne(subset_ratio=0.2, apply_pca=True, n_components=50, save_path=tsne_vis_path)\n",
    "    \n",
    "#     # Save the labeled dataset\n",
    "#     preprocessor.save_labeled_dataset(output_path)\n",
    "    \n",
    "#     # Print metrics summary\n",
    "#     metrics_summary = preprocessor.get_metrics_summary()\n",
    "#     print(\"\\nMetrics Summary:\")\n",
    "#     for key, value in metrics_summary.items():\n",
    "#         print(f\"{key}: {value}\")\n",
    "    \n",
    "#     # Verify saved data\n",
    "#     with np.load(output_path) as data:\n",
    "#         saved_images = data['images']\n",
    "#         saved_labels = data['labels']\n",
    "#         print(f\"Saved data: images shape {saved_images.shape}, labels shape {saved_labels.shape}\")\n",
    "\n",
    "# processImages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108071e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def postProcessImages():\n",
    "    npz_path = '../datasets/labeled_train_data.npz'\n",
    "    output_path = '../datasets/updated_labeled_train_data.npz'\n",
    "    vis_path = '../datasets/current_labels_visualization.png'\n",
    "    \n",
    "    modifier = ImageLabelModifier(npz_path, random_state=42)\n",
    "    \n",
    "    # Load the .npz file\n",
    "    images, labels = modifier.load_npz_data()\n",
    "    \n",
    "    # Visualize current labels to inspect clusters\n",
    "    modifier.visualize_current_labels(samples_per_cluster=5, save_path=vis_path)\n",
    "    \n",
    "    # Example: Define a cluster-to-digit mapping based on visualization\n",
    "    # This is a placeholder; replace with actual mapping after inspecting visualization\n",
    "    cluster_to_digit_map = {\n",
    "        0: 1,  # Cluster 0 corresponds to digit 5\n",
    "        1: 6,  # Cluster 1 corresponds to digit 2\n",
    "        2: 2,  # Cluster 2 corresponds to digit 1\n",
    "        3: 0,  # Cluster 3 corresponds to digit 7\n",
    "        4: 3,  # Cluster 4 corresponds to digit 4\n",
    "        5: 8,  # Cluster 5 corresponds to digit 0\n",
    "        6: 9,  # Cluster 6 corresponds to digit 9\n",
    "        7: 4,  # Cluster 7 corresponds to digit 3\n",
    "        8: 1,  # Cluster 8 corresponds to digit 6\n",
    "        9: 0   # Cluster 9 corresponds to digit 8\n",
    "    }\n",
    "    \n",
    "    # Apply the mapping\n",
    "    updated_labels = modifier.map_cluster_to_digit(cluster_to_digit_map)\n",
    "    \n",
    "    # Optional: Manually correct a few samples\n",
    "    # updated_labels = modifier.manually_correct_samples(n_samples=10)\n",
    "    \n",
    "    # Visualize updated labels to confirm changes\n",
    "    modifier.visualize_current_labels(samples_per_cluster=5, save_path=vis_path.replace('current', 'updated'))\n",
    "    \n",
    "    # Save the updated dataset\n",
    "    modifier.save_updated_dataset(output_path)\n",
    "    \n",
    "    # Verify saved data\n",
    "    with np.load(output_path) as data:\n",
    "        saved_images = data['images']\n",
    "        saved_labels = data['labels']\n",
    "        print(f\"Saved data: images shape {saved_images.shape}, labels shape {saved_labels.shape}\")\n",
    "\n",
    "\n",
    "def postProcessImages():\n",
    "    npz_path = '../datasets/labeled_train_data.npz'\n",
    "    output_path = '../datasets/updated_labeled_train_data.npz'\n",
    "    \n",
    "    modifier = ImageLabelModifier(npz_path, random_state=42)\n",
    "    \n",
    "    # Iteratively update labels\n",
    "    updated_labels = modifier.iterative_update_labels(samples_per_cluster=5, vis_path_prefix='labels_visualization')\n",
    "    \n",
    "    # Save the updated dataset\n",
    "    modifier.save_updated_dataset(output_path)\n",
    "    \n",
    "    # Verify saved data\n",
    "    with np.load(output_path) as data:\n",
    "        saved_images = data['images']\n",
    "        saved_labels = data['labels']\n",
    "        print(f\"Saved data: images shape {saved_images.shape}, labels shape {saved_labels.shape}\")\n",
    "\n",
    "postProcessImages()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
